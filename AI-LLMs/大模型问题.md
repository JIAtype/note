《Attention is All You Need》
吴恩达《LLM微调专项课程》，掌握LoRA/P-Tuning等适配器技术。
数据集处理：完成GLUE基准数据集预处理，重点练习文本清洗、token对齐和数据增强技巧。
训练优化：使用Weights & Biases监控训练过程，记录loss曲线和GPU利用率分析报告。
架构缩写（如MoE, KV Cache）
建立双链笔记库，用Obsidian关联Transformer-XL到RetNet等演进路径。
💻 分布式训练专项
并行策略：用3天掌握Megatron-LM框架，理解TP/PP/DP三种并行方式的混合配置。
故障调试：每天练习1个NCCL通信错误案例，使用PyTorch Profiler定位瓶颈。
性能优化：在AWS p4d实例上完成吞吐量压测，调整gradient_accumulation_steps参数寻找最优值。
①LLM学习路线图（2024版）
②《大规模预训练模型》课程课件（含代码）
③Hugging Face技术文档中文版
④LLM实战数据集包（含SFT/RLHF）
⑤经典论文合集（2017-2024顶会精选）
⑥分布式训练调试手册
⑦Transformer可视化工具集
⑧模型量化工具包（AWQ+GGUF）
⑨提示工程模版库（200+场景）
⑩大模型推理优化checklist

# 为什么现在的大模型都是decoder-only结构

在大规模语言模型的发展历程中，最早的Transformer结构是“Encoder-Decoder”双器结构，然而以 GPT 系列为代表的模型在实践中普遍采用了“Decoder-only”结构，并逐渐成为主流。之所以许多主流大模型（GPT-3、GPT-4、Llama 等）都选择采用 Decoder-only 架构，主要原因可以概括为以下几点：

1. 更贴合自回归生成任务  
   • 大模型常见的应用场景是文本生成，例如问答、写作、对话等。Decoder-only 模型天然对应自回归（auto-regressive）生成任务：它只需要根据已有上下文来预测下一个词（token），并在不断重复这个过程后生成整段文本。  
   • 由于自回归预测只需要关注“直到当前时刻已生成的所有词”，Decoder-only 模型不必像 Encoder-Decoder 结构那样额外维护一个固定长度的编码结果，模型结构更简单，也与自回归训练目标更契合。

2. 训练与部署的简化  
   • 在超大规模预训练时，Decoder-only 结构由于只需要考虑单向的掩码（causal mask），不需要双向关注机制和额外的跨注意力机制（cross-attention），因此在工程实现和硬件适配上相对更简化。  
   • Encoder-Decoder 结构涉及到编码器输出与解码器的跨注意力（cross attention）部分，结构更复杂，也会增加训练和推理过程中的计算负担。

3. 模型可扩展性和表现  
   • Decoder-only 模型可以利用极大规模的文本数据进行自回归训练，在大规模数据上已经表现出很强的学习和推断能力。  
   • 举例来说，GPT-3、GPT-4 这类大规模 Decoder-only 模型在零样本/小样本（zero-shot/few-shot）学习中展现了很强的任务迁移能力，证明了纯自回归训练对语言理解与生成的有效性。

4. 适用于多种场景的“通用性”  
   • 虽然 Encoder-Decoder 在机器翻译等需要明确“输入–输出”映射的场景中非常有效，但在通用的语言生成、对话、总结、推理等任务上，大规模的 Decoder-only 也能有很好的适配能力。  
   • 在应用层面，许多企业和组织希望统一使用一种“通用”语言模型来覆盖问答、续写、摘要、推理等多种场景，Decoder-only 架构在这一方面拥有足够的灵活性。

需要补充的是，并不是所有的大模型都采用了 Decoder-only 结构。以 T5、BART 等为代表的模型依然沿用 Encoder-Decoder 结构，并在特定任务（如机器翻译、文本摘要、特定的下游任务）中取得了非常优异的表现。但从行业趋势看，基于自回归方式进行大规模预训练、并将其用于通用语言理解和生成，是当前大模型的一大主要范式，因此常见的 GPT 系列（即 Decoder-only）在技术和应用上都更为普及。

---

让我为您详细解释Transformer的核心结构和不同模型的对比：

1. Transformer核心结构：

a) Self-Attention机制：
- 计算过程：
  Q = XW_Q, K = XW_K, V = XW_V
  Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V
- 复杂度：O(n²d)，其中n是序列长度，d是特征维度
- 优点：可以并行计算，捕捉长距离依赖

b) Feed-Forward Neural Network (FFN)：
- 结构：两层全连接层
  FFN(x) = max(0, xW1 + b1)W2 + b2
- 复杂度：O(n·d²)，其中d是特征维度
- 作用：引入非线性变换

c) Layer Normalization：
- 计算：对每个样本的特征维度进行归一化
- 作用：稳定训练过程，加速收敛

d) 残差连接：
- 结构：x + Transformer(x)
- 作用：缓解梯度消失，便于训练深层网络

2. 不同模型的对比：

a) GPT (Decoder-only)：
- 结构：纯解码器结构
- 计算复杂度：O(n²d)
- 优点：
  - 简单高效，易于并行化
  - 适合自回归生成任务
  - 零样本/少样本学习能力强
- 适用场景：
  - 文本生成
  - 对话系统
  - 代码生成
  - 通用语言理解

b) BERT (Encoder-only)：
- 结构：纯编码器结构，使用masked language modeling
- 计算复杂度：O(n²d)
- 优点：
  - 双向上下文理解
  - 适合理解任务
- 适用场景：
  - 语义理解
  - 文本分类
  - 命名实体识别
  - 问答系统

c) T5 (Encoder-Decoder)：
- 结构：完整的编码器-解码器结构
- 计算复杂度：O(n²d) + O(m²d)，m为输出序列长度
- 优点：
  - 适合序列到序列任务
  - 可以处理多种任务格式
- 适用场景：
  - 机器翻译
  - 文本摘要
  - 文本改写
  - QA系统

3. 复杂度计算：

a) 时间复杂度：
- Self-Attention: O(n²d)
- FFN: O(n·d²)
- 总体：O(n²d) + O(n·d²)

b) 空间复杂度：
- 参数量：O(d²)
- 内存：O(n² + nd)

4. 选择模型的考虑因素：

- 任务类型：
  - 纯生成任务：GPT
  - 纯理解任务：BERT
  - 序列到序列任务：T5

- 资源限制：
  - 计算资源有限：GPT
  - 需要双向理解：BERT
  - 复杂任务：T5

- 性能要求：
  - 高效生成：GPT
  - 精确理解：BERT
  - 综合性能：T5

总的来说，选择哪种模型主要取决于具体应用场景的需求：
- 如果需要快速生成文本，GPT是最佳选择
- 如果需要精确理解文本，BERT更合适
- 如果需要处理复杂的序列到序列任务，T5是更好的选择

每种模型都有其独特的优势和适用场景，选择时需要根据具体任务需求和资源限制来决定。