# Transformer基本结构？

# 解释自注意力机制

# 输入向量的维度是多少？

# 为什么在计算注意力时要？
防止梯度消失，稳定训练

# LayerNorm是对哪个维度做归一化？
seqlen维度，即对每个token的embedding做归一化

