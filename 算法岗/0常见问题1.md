Transformer中为什么用LN不用BN？

图像中BN是怎么用的？

在NLP中如果句子长度不一致，用BN会有什么后果？

给定三维矩阵，BN和LN分别作用在哪个维度？

已知bsz seq_len dim head,参数量是多少，和哪几个参数相关？

带有多个注意力头的注意力机制计算过程？

说出pytorch中维度变换的函数

显存OOM，参数，ZERO，vllm，梯度累积，优化器，混合精度

长度外推技术

LongLoRA和LoRA的区别

LLM的长度扩展策略有哪些

介绍YaRN

幻觉怎么判断出来？如何解决？

AI Agent

RAG

910B适配过程中遇到的问题

怎么提高下游任务的效果

Layer normalization

Attention

LLMs损失函数

7种大模型微调方法

大模型Langchain

基于Langchain的RAG问答

大模型强化学习

LLM激活函数

Transformer操作

相似度函数

多轮对话中让AI保持长期记忆

大模型外挂知识库优化

大模型参数高效微调

提示学习Prompting

如何使用PEFT库中的LoRA？

大模型增量预训练

基于lora的llama二次预训练

RLHF

# 训练过程中Transformer更新哪些参数？

# 如何缓解大模型的幻觉问题？

# 如何让模型处理更长的文本？
位置内插YoRN
NTK-aware缩放
RoPE扩展等

旋转位置编码RoPE如何提升外推性
除了RoPE，还有哪些长文本扩展技术

# 大模型的解释性和公平性

# 优化LLM的检索效果
优化索引
特征工程
模型选择

# 