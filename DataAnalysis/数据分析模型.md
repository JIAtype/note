	适用数据	作用	介绍	特点	缺点	使用方法
ZeroR	任何数据，但仅用于基准测试。	作为基准模型，用于与其他模型对比。	仅预测目标变量的众数（分类）或均值（回归）。	最简单的模型	性能通常较差，无法捕捉数据中的模式。	直接调用模型，无需调参。
RXGBoost	结构化数据（如表格数据）。	分类和回归任务。	梯度提升树（Gradient Boosting）	高效、可扩展，支持并行计算。	对超参数敏感，训练时间较长。	调参（如n_estimators, max_depth）。
LightGBM	大规模结构化数据	分类和回归任务	基于直方图的决策树算法，梯度提升树。	训练速度快，内存占用低。	对小数据集可能过拟合。	调参（如num_leaves, learning_rate）
RCaretGBM	结构化数据。	分类和回归任务。	梯度提升树。	R语言中caret包实现的GBM模型。	性能略低于XGBoost和LightGBM。	通过caret包调用，调参（如n.trees, interaction.depth）。
RLassoGlmNet	高维数据。	回归任务。	线性回归（L1正则化）。	通过L1正则化进行特征选择。	对共线性数据效果较差。	调参（如alpha）。
RRidgeGlmNet	共线性数据。	回归任务。	线性回归（L2正则化）。	通过L2正则化防止过拟合。	无法进行特征选择。	调参（如alpha）。
RRandomForest	结构化数据。	分类和回归任务。	集成学习（Bagging）。	基于决策树的集成模型，抗过拟合能力强。	训练时间较长，对高维稀疏数据效果较差。	调参（如n_estimators, max_depth）。
SciKitLearnSVM	小样本、高维数据。	分类和回归任务。	支持向量机（SVM）。	适合小样本、高维数据，核函数可处理非线性问题。	训练时间较长，对大规模数据不适用。	调参（如C, kernel）。
StatsModelsGLM	特定分布数据。	回归任务	广义线性模型（GLM）。	适用于多种分布（如正态、泊松、二项分布）。	对非线性数据效果较差。	选择适合的分布（如family=Gaussian()）。
SciKitLearnSGD	大规模数据。	分类和回归任务。	随机梯度下降（SGD）。	支持多种损失函数。	对超参数敏感，训练不稳定。	调参（如loss, alpha）。
SciKitLearnRidge	共线性数据。	回归任务。	线性回归（L2正则化）。	与RidgeGlmNet类似，但基于scikit-learn实现。	无法进行特征选择。	调参（如alpha）。
SciKitLearnLasso	高维数据	回归任务。	线性回归（L1正则化）。	与LassoGlmNet类似，但基于scikit-learn实现。	对共线性数据效果较差。	调参（如alpha）。
KerasDeepLearning	图像、文本、时间序列等复杂数据。	复杂非线性问题	深度学习。	适合处理复杂非线性问题。	训练时间长，需要大量数据。	设计神经网络结构，调参（如epochs, batch_size）。
RRpartDecisionTree	结构化数据	分类和回归任务。	决策树。	R语言中rpart包实现的决策树模型。	容易过拟合。	调参（如cp, maxdepth）。
SciKitLearnBagging	结构化数据。	分类和回归任务。	集成学习（Bagging）。	基于scikit-learn的Bagging模型，适合高方差模型。	训练时间较长。	调参（如n_estimators, max_samples）。
SciKitLearnLassoCV	高维数据。	回归任务。	线性回归（L1正则化，交叉验证）。	通过交叉验证自动选择正则化参数。	对共线性数据效果较差。	直接调用，无需手动调参。
SciKitLearnXGBoost	结构化数据。	分类和回归任务。	梯度提升树。	基于scikit-learn接口的XGBoost模型。	对超参数敏感，训练时间较长。	调参（如n_estimators, max_depth）。
SciKitLearnAdaBoost	结构化数据。	分类和回归任务。	集成学习（Boosting）。	基于scikit-learn的AdaBoost模型，适合弱分类器集成。	对噪声数据敏感。	调参（如n_estimators, learning_rate）。
SciKitLearnElasticNet	高维数据。	回归任务。	线性回归（L1+L2正则化）。	结合L1和L2正则化，适合高维数据。	对超参数敏感。	调参（如alpha, l1_ratio）。
SciKitLearnLassoLarsCV	高维数据。	回归任务。	线性回归（L1正则化，LARS算法，交叉验证）。	通过LARS算法和交叉验证选择正则化参数。	对共线性数据效果较差。	直接调用，无需手动调参。
SciKitLearnDecisionTree	结构化数据。	分类和回归任务。	直接调用，无需手动调参。	基于scikit-learn的决策树模型。	容易过拟合。	调参（如max_depth, min_samples_split）。
SciKitLearnRandomForest	结构化数据。	分类和回归任务。	集成学习（Bagging）。	基于scikit-learn的随机森林模型。	训练时间较长，对高维稀疏数据效果较差。	调参（如n_estimators, max_depth）。
BaggingQuantileRegressor	结构化数据。	回归任务，需要预测分位数。	集成学习（Bagging）。	基于Bagging的分位数回归模型。	训练时间较长。	调参（如n_estimators, max_samples）。
SciKitLearnNuSVMRegressor	小样本、高维数据。	回归任务。	支持向量机（Nu-SVM）。	基于scikit-learn的Nu-SVM回归模型。	训练时间较长，对大规模数据不适用。	调参（如nu, kernel）。
StatsModelsGLMRegularized	特定分布数据。	回归任务，特定分布数据。	广义线性模型（正则化）。	基于statsmodels的正则化GLM模型。	对非线性数据效果较差。	选择适合的分布（如family=Gaussian()）。
SciKitLearnGradientBoosting	结构化数据。	分类和回归任务。	梯度提升树。	基于scikit-learn的梯度提升树模型。	对超参数敏感，训练时间较长。	调参（如n_estimators, learning_rate）。
SciKitLearnLinearRegression	线性关系数据。	回归任务。	线性回归。	基于scikit-learn的普通线性回归模型。	对非线性数据效果较差。	直接调用，无需调参。
Stack(XGBoost,LinearRegression)	结构化数据。	分类和回归任务。	堆叠模型（Stacking）。	将XGBoost和线性回归模型堆叠，提高预测性能。	训练时间较长，模型复杂度高。	定义基模型和元模型，调参（如n_estimators, alpha）。
SciKitLearnElasticNetCVRegressor	高维数据。	回归任务。	线性回归（L1+L2正则化，交叉验证）。	通过交叉验证自动选择正则化参数。	对超参数敏感。	直接调用，无需手动调参。
SortedStack(XGBoost,LinearRegression)	结构化数据。	分类和回归任务。	堆叠模型（排序堆叠）。	在堆叠基础上对模型输出进行排序。	训练时间较长，模型复杂度高。	定义基模型和元模型，调参（如n_estimators, alpha）。
BoundedStack(XGBoost,LinearRegression)	结构化数据。	分类和回归任务。	堆叠模型（有界堆叠）。	在堆叠基础上对模型输出进行有界处理。	训练时间较长，模型复杂度高。	定义基模型和元模型，调参（如n_estimators, alpha）。
SciKitLearnLogisticRegressionClassifier	线性可分数据。	分类任务	逻辑回归。	基于scikit-learn的逻辑回归模型。	对非线性数据效果较差。	调参（如C, penalty）。
SciKitLearnLogisticRegressionCVClassifier	线性可分数据。	分类任务	逻辑回归（交叉验证）。	通过交叉验证自动选择正则化参数。	对非线性数据效果较差。	直接调用，无需手动调参。
SciKitLearnGradientBoostingQuantileRegressor	结构化数据。	回归任务，需要预测分位数。	梯度提升树（分位数回归）。	基于scikit-learn的分位数回归模型。	训练时间较长。	调参（如n_estimators, learning_rate）。
BoundedSortedStack(XGBoost,LinearRegression)	结构化数据。	分类和回归任务。	堆叠模型（有界排序堆叠）。	结合有界处理和排序的堆叠模型。	训练时间较长，模型复杂度高。	定义基模型和元模型，调参（如n_estimators, alpha）。
						
						
						
梯度提升树（Gradient Boosting Tree, GBT）是一种强大的机器学习算法，属于集成学习（Ensemble Learning）中的Boosting方法。它通过逐步构建多个弱学习器（通常是决策树），并将它们组合成一个强学习器，从而提高模型的预测性能。						
支持向量机（Support Vector Machine, SVM）是一种强大的监督学习算法，主要用于分类和回归任务。它的核心思想是通过找到一个最优超平面，将不同类别的数据分开，同时最大化分类边界（即“间隔”）。						
广义线性模型（Generalized Linear Model, GLM）是线性回归的扩展，适用于多种类型的响应变量（如连续值、二分类、计数等）。它通过引入链接函数和指数族分布，将线性模型推广到更广泛的应用场景。						
						