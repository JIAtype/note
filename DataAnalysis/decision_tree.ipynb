{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X是要分析的列，需要drop target列和不相关的列\n",
    "y是目标列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['target','trayno'],axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建决策树模型，打印准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"准确率: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification_report 分类报告\n",
    "\n",
    "准确率（Accuracy）:\n",
    "模型在所有样本中预测正确的比例。\n",
    "\n",
    "每个类别的指标\n",
    "\n",
    "* precision（精确率）：预测为正类的样本中，实际为正类的比例。\n",
    "* recall（召回率）：实际为正类的样本中，预测为正类的比例。\n",
    "* f1-score（F1 分数）：精确率和召回率的调和平均值。\n",
    "* support（支持数）：每个类别的样本数量。\n",
    "样本数量接近，说明数据集较为平衡。\n",
    "\n",
    "整体指标\n",
    "\n",
    "* accuracy（准确率）：模型在所有样本中预测正确的比例。\n",
    "* macro avg（宏平均）：所有类别的指标的平均值。\n",
    "* weighted avg（加权平均）：根据每个类别的样本数量加权计算的指标平均值。\n",
    "\n",
    "适用性：如果测试集与真实数据分布一致，模型在实际应用中可能表现良好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征重要性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model):\n",
    "    importance = model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # 创建特征重要性DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "\n",
    "    # 按重要性排序\n",
    "    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "    print(\"参数重要性:\")\n",
    "    print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征重要性\n",
    "importance = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# 创建特征重要性DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "# 按重要性排序\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for Quality Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印前几个最重要的特征\n",
    "print(\"最重要的参数:\")\n",
    "print(feature_importance_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# 可视化决策树\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(model, feature_names=feature_names, \n",
    "               class_names=['ac', 'nc'], \n",
    "               filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树分析\n",
    "\n",
    "出现在较高层的特征通常更重要，因为它们对样本的分割影响更大。\n",
    "\n",
    "节点结构，第一个节点为例子，解释信息：\n",
    "\n",
    "* 特征分割条件：duration_type<=31.5，表示根据duration_type的值是否小于等于31.5进行分割。\n",
    "* 基尼指数（Gini Index）：gini = 0.3，表示节点的不纯度。\n",
    "\n",
    "    gini 值越小：表示节点越纯净，样本更倾向于属于同一类别。\n",
    "\n",
    "    gini 值越大：表示节点越不纯，样本分布越均匀。\n",
    "    \n",
    "* 样本数量：samples=989，表示当前节点包含 989 个样本。\n",
    "* 类别分布：value=[60, 40]，表示当前节点中类别 A 有 469 个样本，类别 B 有 520 个样本。\n",
    "* 类别标签：class=nc，表示当前节点的预测类别。\n",
    "\n",
    "颜色填充\n",
    "\n",
    "* 颜色深浅：颜色越深，表示当前节点的纯度越高（即某一类别的样本占比越大）。\n",
    "* 颜色对应类别：不同颜色代表不同类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 优化模型\n",
    "\n",
    "# 剪枝：避免过拟合\n",
    "# 使用剪枝参数构建更稳健的模型\n",
    "optimized_model = DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# 交叉验证：提高模型稳定性\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(optimized_model, X, y, cv=5)\n",
    "print(f\"交叉验证平均准确率: {np.mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
