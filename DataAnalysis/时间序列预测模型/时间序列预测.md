时间序列任务：
1.短预测：预测未来某个时刻的值
2.长预测：预测未来某个时刻的趋势
3.异常点识别，逐点二分类
4.分类任务
5.序列补全

经典模型LSTM
序列预测中，序列越长，速度越慢，效果越差。

Transformer
万能模板，直接套用。
利用权重，丰富上下文特征。
并行，比LSTM块，全局信息丰富，注意力机制好。
长序列中attention需要每一个点跟其他点计算（如果序列太长，效率很低）
Decoder要基于上一个预测结果来推断当前的预测结果

三个方法：
1.Informer
权重非常小的点，没什么用，直接删掉也不会影响分析结果。就不用了
长尾效应的点不要了。
长序列中采样：不干活（没什么影响的特征）不用，直接删掉。
2.TimesNet模型
五边形战士，所有类型问题表现都很好。

第一步：一维变二维（用FFT傅里叶变换展开）
FFT傅里叶变换，一个因素拆分后可以看到各种小点的结合。
每件事的权重不一样，衡量重要性。
第二步：得到Top k
找到最重要的k点影响因素
直接用其本身的幅值，也有权重。
第三步：做卷积
reshape
padding
例子，三个维度：batch10 乘 length100 乘 feature10
1.用FFT傅里叶变换展开 
T=2 结果 => 10 乘 2乘50 乘 10
T=3 结果 => 10 乘 3乘34 乘 10 （超过100，所以做填充reshape）
T=4 结果 => 10 乘 4乘25 乘 10
第四步：提特征
多层去做。可以反复执行。
一维展开成二维，二维叠成一维。一维再展开，二维再叠。反复做

3.大模型做时间序列分析
第一步：让大模型识别出时间序列特征
我们要做：原始数据拆成几个区间，看每个区间的特征
我们可以：根据每个区间的特征，把这些特征用文本描述出来，给大模型

全连接层，把词映射成与时序相关的信息。
算patch之间的联系。用内积or交叉注意力机制

文本域到时间域的映射

源域到目标域的映射

第二步：